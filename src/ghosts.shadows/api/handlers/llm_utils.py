from typing import List, Optional

import ollama

from .filters import filter_llm_response


def generate_response(
    query: str, model_name: str, documents: Optional[List] = None
) -> str:
    """
    Generate a response from the LLM model, using document embeddings to add relevant context.
    If no documents are provided, the query is processed without additional context.

    Args:
        query (str): The user's query.
        model_name (str): The name of the model to use for generating the response.
        documents (Optional[List[str]], optional): The list of documents to embed and compare with the query.
            If not provided, defaults to None.

    Returns:
        str: The filtered response generated by the LLM.
    """
    try:
        if documents:
            query_with_context = f"Context: {documents}\n\nUser Query: {query}"
        else:
            query_with_context = f"User Query: {query}"

        response = ollama.chat(
            model=model_name,
            messages=[{"role": "user", "content": query_with_context}],
        )
        llm_response = response["message"]["content"]

        return filter_llm_response(llm_response)
    except Exception as e:
        return f"Error generating response: {str(e)}"
